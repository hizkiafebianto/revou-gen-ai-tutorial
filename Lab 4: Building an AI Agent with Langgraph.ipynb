{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71024a1b",
   "metadata": {},
   "source": [
    "# Lab 4: Building an AI Agent with Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1d012",
   "metadata": {},
   "source": [
    "## LangGraph \n",
    "Langgraph is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables **agent orchestration** — offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.\n",
    "\n",
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing langgraph package\n",
    "!pip install langgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07024fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading environment variables \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)  # take environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7c482",
   "metadata": {},
   "source": [
    "## Learning about Graph\n",
    "\n",
    "Let's examine the simplest form of a graph as used in LangGraph. <br /><br />\n",
    "<img src=\"./assets/simple graph.png\" width=\"450\">\n",
    "\n",
    "A LangGraph graph is composed of the following components:\n",
    "1. State: The state represents the shared memory or context that flows through the graph. It is typically a structured data object (e.g., a dictionary or class) that holds intermediate inputs, outputs, and metadata across the graph's execution. The state is updated as it moves from node to node.\n",
    "\n",
    "2. Node: A node represents a unit of computation — usually a function, language model call, or tool execution. Each node operates on the incoming state, performs some processing, and returns an updated state. Nodes are the core logic blocks in LangGraph.\n",
    "\n",
    "3. Edge: An edge defines the transition between nodes. It determines which node to execute next based on the current state or the output of the previous node. Edges can be static (predefined paths) or dynamic (based on conditions or branching logic), enabling flexible and adaptive workflows.\n",
    "\n",
    "### State, Nodes, and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23117b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State\n",
    "from typing import TypedDict, Literal\n",
    "\n",
    "class State(TypedDict):\n",
    "    user_selection: str\n",
    "    graph_state: str\n",
    "\n",
    "# Nodes\n",
    "# Nodes are just python functions. Each node operates on the state.\n",
    "# Langgraph has 2 special nodes, called START node and END, to denote the start and the end of a graph.\n",
    "\n",
    "def node1(state):\n",
    "    print(\"---Inside Node No 1---\")\n",
    "    return {'graph_state': state['graph_state'] + \"Passing through Node 1. | \"}\n",
    "\n",
    "def node2(state):\n",
    "    print(\"---Inside Node No 2---\")\n",
    "    return {'graph_state': state['graph_state'] + \"Passing through Node 2. | \"}\n",
    "\n",
    "def node3(state):\n",
    "    print(\"---Inside Node No 3---\")\n",
    "    return {'graph_state': state['graph_state'] + \"Passing through Node 3. | \"}\n",
    "\n",
    "# Edges\n",
    "# 2 types of edges: normal edges vs conditional edges\n",
    "# Normal edges will be defined directly when we build the graph using langgraph\n",
    "# Conditional edges require a function to define the conditions\n",
    "# Here we will define an example of conditional edges\n",
    "\n",
    "def decide_your_way(state) -> Literal[\"Node2\", \"Node3\"]:\n",
    "    user_selection = state['user_selection']\n",
    "\n",
    "    if user_selection == \"Node 2\":\n",
    "        return \"Node2\"\n",
    "    else:\n",
    "        return \"Node3\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03a6c1",
   "metadata": {},
   "source": [
    "### Graph Construction\n",
    "Let's connect those nodes into a graph using StateGraph from LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b202af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Build the nodes\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"Node1\", node1)\n",
    "builder.add_node(\"Node2\", node2)\n",
    "builder.add_node(\"Node3\", node3)\n",
    "\n",
    "# Connect Edges\n",
    "builder.add_edge(START, \"Node1\")\n",
    "builder.add_conditional_edges(\"Node1\", decide_your_way)\n",
    "builder.add_edge(\"Node2\", END)\n",
    "builder.add_edge(\"Node3\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3f58f",
   "metadata": {},
   "source": [
    "### Graph Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a229ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph invocation\n",
    "user_input = input(\"Decide which way you want to go (Node 2 or Node 3): \")\n",
    "graph.invoke({'user_selection': user_input, 'graph_state': \"START | \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c461f",
   "metadata": {},
   "source": [
    "## State Schema\n",
    "\n",
    "When you initiate a builder in LangGraph, you need to provide the schema of the graph's state to the StateGraph class. This schema defines the structure and data types of the state. Previously, the schema was created using the TypedDict class, but it can also be defined using Python's dataclass or the Pydantic library. Using Pydantic allows you to enforce type validation, offering better protection for the state's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a22aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous schema, but we put limitation on user_selection parameter to only valid for 2 values only\n",
    "class TypedDictState(TypedDict):\n",
    "    user_selection: Literal[\"Node 2\", \"Node 3\"]\n",
    "    graph_state: str\n",
    "\n",
    "# modify the conditional node \n",
    "def choose_paths(state):\n",
    "    user_selection = state['user_selection']\n",
    "\n",
    "    if user_selection == \"Node 2\":\n",
    "        return \"Node2\"\n",
    "    elif user_selection == \"Node 3\":\n",
    "        return \"Node3\"\n",
    "    else: \n",
    "        return END\n",
    "\n",
    "# Build the nodes\n",
    "builder = StateGraph(TypedDictState)\n",
    "builder.add_node(\"Node1\", node1)\n",
    "builder.add_node(\"Node2\", node2)\n",
    "builder.add_node(\"Node3\", node3)\n",
    "\n",
    "# Connect Edges\n",
    "builder.add_edge(START, \"Node1\")\n",
    "builder.add_conditional_edges(\"Node1\", choose_paths)\n",
    "builder.add_edge(\"Node2\", END)\n",
    "builder.add_edge(\"Node3\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph2 = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36576db",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Decide which way you want to go (Node 2 or Node 3): \")\n",
    "graph2.invoke({'user_selection': user_input, 'graph_state': \"START | \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pydantic\n",
    "from langchain_core.pydantic_v1 import BaseModel, ValidationError\n",
    "\n",
    "class PydanticState(BaseModel):\n",
    "    user_selection: Literal[\"Node 2\", \"Node 3\"]\n",
    "    graph_state: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PydanticState(user_selection=\"Node 4\", graph_state=3) \n",
    "PydanticState(user_selection=\"Node 2\", graph_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PydanticState(user_selection=\"Node 2\", graph_state=\"fi\") \n",
    "a.user_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6beb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the nodes\n",
    "def node1(state):\n",
    "    print(\"---Inside Node No 1---\")\n",
    "    return {'graph_state': state.graph_state + \"Passing through Node 1. | \"}\n",
    "\n",
    "def node2(state):\n",
    "    print(\"---Inside Node No 2---\")\n",
    "    return {'graph_state': state.graph_state + \"Passing through Node 2. | \"}\n",
    "\n",
    "def node3(state):\n",
    "    print(\"---Inside Node No 3---\")\n",
    "    return {'graph_state': state.graph_state + \"Passing through Node 3. | \"}\n",
    "\n",
    "def choose_paths(state):\n",
    "    user_selection = state.user_selection\n",
    "\n",
    "    if user_selection == \"Node 2\":\n",
    "        return \"Node2\"\n",
    "    elif user_selection == \"Node 3\":\n",
    "        return \"Node3\"\n",
    "    else: \n",
    "        return END\n",
    "    \n",
    "builder = StateGraph(PydanticState)\n",
    "builder.add_node(\"Node1\", node1)\n",
    "builder.add_node(\"Node2\", node2)\n",
    "builder.add_node(\"Node3\", node3)\n",
    "\n",
    "# Connect Edges\n",
    "builder.add_edge(START, \"Node1\")\n",
    "builder.add_conditional_edges(\"Node1\", choose_paths)\n",
    "builder.add_edge(\"Node2\", END)\n",
    "builder.add_edge(\"Node3\", END)\n",
    "\n",
    "# Compile graph\n",
    "graph3 = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1da6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Decide which way you want to go (Node 2 or Node 3): \")\n",
    "\n",
    "graph_state = None\n",
    "try: \n",
    "    graph_state = graph3.invoke({'user_selection': user_input, 'graph_state': \"START | \"})\n",
    "except ValidationError as e:\n",
    "    print(e)\n",
    "\n",
    "print(graph_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4e88a",
   "metadata": {},
   "source": [
    "## Message Reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181ee87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello, How can I help you?', additional_kwargs={}, response_metadata={}, name='GPT', id='c8e720bb-9958-4f13-9665-e7e0e032b11e'),\n",
       " HumanMessage(content=\"I'm learning about generative AI, please explain about it.\", additional_kwargs={}, response_metadata={}, name='Hizkia', id='95ecba6a-e37e-4216-b235-67149cda2a4f'),\n",
       " AIMessage(content='Sure, I can help you with that. Here is a brief explanation about generative AI. Generative AI is.... ', additional_kwargs={}, response_metadata={}, name='GPT4', id='b754fea3-a85b-4720-abaf-22bc65d929a7')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MessagesState \n",
    "# Built-in state for working with messages \n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Append Messages into the state\n",
    "initial_state = [AIMessage(content=\"Hello, How can I help you?\", name = \"GPT\"),\n",
    "                 HumanMessage(content=\"I'm learning about generative AI, please explain about it.\", name = \"Hizkia\")\n",
    "                 ]\n",
    "\n",
    "new_message = AIMessage(content=\"Sure, I can help you with that. Here is a brief explanation about generative AI. Generative AI is.... \", name = \"GPT4\")\n",
    "\n",
    "# append\n",
    "add_messages(initial_state, new_message)\n",
    "\n",
    "# the MessagesState Class has embedded add_messages function so a new message will be automatically appended into the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting Messages \n",
    "# use id \n",
    "\n",
    "initial_state = [AIMessage(content=\"Hello, How can I help you?\", name = \"GPT\", id = 1),\n",
    "                 HumanMessage(content=\"I'm learning about generative AI, please explain about it.\", name = \"Hizkia\", id = 2)\n",
    "                 ]\n",
    "\n",
    "new_message = HumanMessage(content=\"I am looking for definition of agentic AI.\", name = \"GPT4\", id = 2)\n",
    "\n",
    "add_messages(initial_state, new_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting Messages \n",
    "\n",
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "# Message List \n",
    "messages = [AIMessage(content=\"Hi, My name is ChatGPT. How may I help you?\", name=\"Bot\", id=1)]\n",
    "messages.append(HumanMessage(content=\"Hi.\", name=\"Hizkia\", id=2))\n",
    "messages.append(AIMessage(content=\"So you said you were looking for information on agentic AI?\", name=\"Bot\", id=3))\n",
    "messages.append(HumanMessage(content=\"Yes, can u provide the brief definition of the term agentic AI?\", name=\"Bot\", id=4))\n",
    "\n",
    "# Delete all but the 2 most recent messages\n",
    "delete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]]\n",
    "print(delete_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_messages(messages, delete_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1bad7",
   "metadata": {},
   "source": [
    "## MessagesState as a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12069dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating Langchain Chat Models\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4.1-mini\", model_provider= \"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60d0cd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m builder.add_edge(\u001b[33m\"\u001b[39m\u001b[33mcall_llm\u001b[39m\u001b[33m\"\u001b[39m, END)\n\u001b[32m     14\u001b[39m graph = builder.compile()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m display(\u001b[43mImage\u001b[49m(graph.get_graph().draw_mermaid_png()))\n",
      "\u001b[31mNameError\u001b[39m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Node to invoke an LLM\n",
    "def call_llm(state: MessagesState):\n",
    "    return {\"messages\": model.invoke(state['messages'])}\n",
    "\n",
    "# build the graph \n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_llm\", call_llm)\n",
    "\n",
    "builder.add_edge(START, \"call_llm\")\n",
    "builder.add_edge(\"call_llm\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b67525de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Hizkia\n",
      "\n",
      "Hi\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "output = graph.invoke({'messages': HumanMessage(content=\"Hi\", name = \"Hizkia\")})\n",
    "\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e87e3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Hizkia\n",
      "\n",
      "Can you explain about agentic AI?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Certainly! \n",
      "\n",
      "**Agentic AI** refers to artificial intelligence systems that possess some degree of agency, meaning they have the capacity to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. Unlike simple AI systems that operate based on predefined rules or reactive responses, agentic AI exhibits goal-directed behavior and can adapt its strategies dynamically.\n",
      "\n",
      "### Key Characteristics of Agentic AI:\n",
      "1. **Autonomy:** Agentic AI operates independently without continuous human intervention.\n",
      "2. **Goal-Orientation:** These systems pursue objectives or tasks, adjusting their actions to meet desired outcomes.\n",
      "3. **Perception and Sensing:** They gather and interpret data from their environment to inform decision-making.\n",
      "4. **Decision-Making:** Agentic AI evaluates possible actions based on predictions or models of outcomes.\n",
      "5. **Learning Ability:** Many agentic AI systems can learn from experience to improve future performance.\n",
      "\n",
      "### Examples:\n",
      "- **Autonomous robots:** Robots that navigate and perform tasks in dynamic environments (e.g., warehouse robots, self-driving cars).\n",
      "- **Personal assistants:** Virtual assistants that plan, learn user preferences, and proactively suggest actions.\n",
      "- **Game-playing agents:** AI that strategizes and adapts in complex games like chess or Go.\n",
      "\n",
      "In research and development, agentic AI is often associated with **multi-agent systems** (where multiple autonomous agents interact) and fields like **reinforcement learning**, which enable agents to learn optimal policies through trial and error.\n",
      "\n",
      "---\n",
      "\n",
      "If you'd like, I can provide more specific examples or delve into the challenges and ethics related to agentic AI!\n"
     ]
    }
   ],
   "source": [
    "output = graph.invoke({'messages': HumanMessage(content=\"Can you explain about agentic AI?\", name = \"Hizkia\")})\n",
    "\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7e11f",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "Let us recreate prompt chaining that we did in Lab 1b, now using Langgraph, instead of using Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0485c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the graph receives an input text, and generates 3 questions and answers based on the input text.\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "input_text = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming industries by enabling machines to learn from data, \n",
    "make decisions, and even improve over time. Applications range from chatbots and virtual assistants \n",
    "to complex data analytics and autonomous vehicles. However, AI also brings challenges such as ethical concerns, \n",
    "bias in algorithms, and job displacement. As AI continues to evolve, balancing innovation with responsible \n",
    "development will be key to its long-term success.\n",
    "\"\"\"\n",
    "\n",
    "# The first node in the chain\n",
    "def summarize(state: MessagesState):\n",
    "    \"\"\"\n",
    "        This tool is called when the user instructs assistant to summarize the passage. Make sure the passage is given by the user before calling this tool. \n",
    "        Ask the user to provide the passage first if you do not find the passage in the conversation. \n",
    "\n",
    "        Receiving the state of the graph as input.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"Summarize the following passage from the user.\")\n",
    "    ] +  state['messages']\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return {'messages': AIMessage(content=response.content, name=\"Bot\", id=response.id)}\n",
    "\n",
    "# The second node in the chain\n",
    "def generate_questions(state: MessagesState):\n",
    "    \"\"\" \n",
    "        This tool is called to generate 3 questions related to a passage. Make sure the passage is given by the user before calling this tool. \n",
    "        Ask the user to provide the passage first if you do not find the passage in the conversation. \n",
    "\n",
    "        Receiving the state of the graph as input.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"Create 3 questions from the passage that the user provides.\")\n",
    "    ] + state['messages']\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return {'messages': AIMessage(content=response.content, name=\"Bot\")}\n",
    "\n",
    "# The third node in the chain \n",
    "def answer_questions(state: MessagesState):\n",
    "    \"\"\" \n",
    "        This tool is usually called right after the generate_questions tool. Make sure the passage is given by the user before calling this tool. \n",
    "        Ask the user to provide the passage first if you do not find the passage in the conversation. \n",
    "\n",
    "        Receiving the state of the graph as input.\n",
    "    \"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"Answer all questions that you previously created based on the passage that the user provides. Format your response in pairs of Question and Answers\")\n",
    "    ] + state['messages']\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return {'messages': AIMessage(content=response.content, name=\"Bot\")}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"summarize_node\", summarize)\n",
    "builder.add_node(\"generate_question_node\", generate_questions)\n",
    "builder.add_node(\"answer_question_node\", answer_questions)\n",
    "builder.add_edge(START, \"summarize_node\")\n",
    "builder.add_edge(\"summarize_node\", \"generate_question_node\")\n",
    "builder.add_edge(\"generate_question_node\", \"answer_question_node\")\n",
    "builder.add_edge(\"answer_question_node\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# user prompt\n",
    "prompt = HumanMessage(content=input_text, name=\"Hizkia\")\n",
    "graph_output = graph.invoke({\"messages\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea85258",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_output['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3645ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in graph_output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08ea45",
   "metadata": {},
   "source": [
    "## Agents with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tools \n",
    "tools = [summarize, generate_questions, answer_questions]\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "# Create Assistant Node\n",
    "\n",
    "def assistant(state: MessagesState):\n",
    "\n",
    "    sys_msg = SystemMessage(content=\"You are a helpful assistant who communicates with the user and decide what the user intends to do. \\n\" \\\n",
    "                            \"When the user says greeting, reponds the greeting politely and introduce yourself as Dexa Smart Assistant.\\n\" \\\n",
    "                            \"When the user instructs you to summarize a passage, ask user to give the passage first, then call the summarize tool.\\n\" \\\n",
    "                            \"When the user asks you to generate questions, ask the user to give you the passage, then call the generate_questions and answer_questions tools sequentially\" \\\n",
    "                            \"When the passage exists in the chat history, directly call the tools.\")\n",
    "    \n",
    "    return {'messages': [model_with_tools.invoke([sys_msg] + state['messages'])]}\n",
    "\n",
    "# Build the graph \n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges \n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show graph \n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628b4120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi, good morning.', additional_kwargs={}, response_metadata={}, id='6406de02-2dfd-4398-b47d-80ff6baafb6f'),\n",
       "  AIMessage(content='Good morning! I am Dexa Smart Assistant. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 9147, 'total_tokens': 9165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6f2eabb9a5', 'id': 'chatcmpl-BgaOBuZM1GwXJcRBRL7LHr1vD1TYI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--02a2dd2a-1b5b-4395-9eba-7f61ab2bf680-0', usage_metadata={'input_tokens': 9147, 'output_tokens': 18, 'total_tokens': 9165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "graph_output = react_graph.invoke({'messages': HumanMessage(content=\"Hi, good morning.\")})\n",
    "# graph_output = react_graph.invoke({'messages': HumanMessage(content=\"Help me summarize a passage.\")})\n",
    "# graph_output = react_graph.invoke({'messages': HumanMessage(content=input_text)})\n",
    "# graph_output = react_graph.invoke({'messages': HumanMessage(content=\"I want to summarize the passage.\")})\n",
    "graph_output\n",
    "\n",
    "### \n",
    "# In the end the assistant does not call any tool because it does not remember the whole conversation. Let's add a memory to the graph so it remembers the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b698c5b",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59cf6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "react_graph_memory = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da152c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a thread \n",
    "config = {\"configurable\": {\"thread_id\" : \"1\"}}\n",
    "\n",
    "# specify an input \n",
    "# messages = HumanMessage(content=\"Hi, good afternoon.\")\n",
    "# messages = HumanMessage(content=\"Help me summarize a passage.\")\n",
    "# messages = HumanMessage(content=input_text) \n",
    "# messages = HumanMessage(content=\"Great! Now can u generate quesions out of it?\")\n",
    "# messages = HumanMessage(content=\"you can use the previous passage.\") \n",
    "messages = HumanMessage(content=\"Now, provide also the answer to those questions.\")\n",
    "\n",
    "# Run \n",
    "messages = react_graph_memory.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']: \n",
    "    m.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f8d75",
   "metadata": {},
   "source": [
    "# Prebuilt ReAct Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "memory2 = InMemorySaver()\n",
    "agent = create_react_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",  \n",
    "    tools=[summarize, generate_questions, answer_questions],  \n",
    "    prompt=\"You are a helpful assistant who communicates with the user and decide what the user intends to do. \\n\" \\\n",
    "            \"When the user says greeting, reponds the greeting politely and introduce yourself as Dexa Smart Assistant.\\n\" \\\n",
    "            \"When the user instructs you to summarize a passage, ask user to give the passage first, then call the summarize tool.\\n\" \\\n",
    "            \"When the user asks you to generate questions, ask the user to give you the passage, then call the generate_questions and answer_questions tools sequentially\" \\\n",
    "            \"When the passage exists in the chat history, directly call the tools.\",\n",
    "    checkpointer=memory2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "config = {\"configurable\": {\"thread_id\" : \"2\"}}\n",
    "# agent.invoke({\"messages\": HumanMessage(content=\"Hi, good evening.\")},config=config)\n",
    "# agent.invoke({\"messages\": HumanMessage(content=f\"Can you help me summarize the following passage: {input_text}\")},config=config)\n",
    "# agent.invoke({\"messages\": HumanMessage(content=\"Please generate questions from the passage.\")},config=config)\n",
    "# agent.invoke({\"messages\": HumanMessage(content=\"Yes, you are correct.\")},config=config)\n",
    "agent.invoke({\"messages\": HumanMessage(content=\"Dont forget to generate the anwer of those questions\")},config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff233d15",
   "metadata": {},
   "source": [
    "## (Bonus) Streaming the graph response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94984ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am Dexa Smart Assistant. We haven't discussed anything yet in this conversation. How can I assist you today?"
     ]
    }
   ],
   "source": [
    "# Create a thread \n",
    "config = {\"configurable\": {\"thread_id\" : \"1\"}}\n",
    "\n",
    "# Start the conversation \n",
    "async for event in react_graph_memory.astream_events({\"messages\": [HumanMessage(content=\"Hi, what have we been talking about?\")]}, config=config, version=\"v2\"):\n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"]['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98b717ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "load_dotenv(override=True)\n",
    "model = ChatAnthropic(model='claude-3-7-sonnet-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "619f9430",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:1457\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1455\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._create(payload)\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1457\u001b[39m     \u001b[43m_handle_anthropic_bad_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_output(data, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:1455\u001b[39m, in \u001b[36mChatAnthropic._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1453\u001b[39m payload = \u001b[38;5;28mself\u001b[39m._get_request_payload(messages, stop=stop, **kwargs)\n\u001b[32m   1454\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1455\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1457\u001b[39m     _handle_anthropic_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:1316\u001b[39m, in \u001b[36mChatAnthropic._create\u001b[39m\u001b[34m(self, payload)\u001b[39m\n\u001b[32m   1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.beta.messages.create(**payload)\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/anthropic/_utils/_utils.py:283\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/anthropic/resources/messages/messages.py:978\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    972\u001b[39m     warnings.warn(\n\u001b[32m    973\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    974\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    975\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/anthropic/_base_client.py:1290\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1278\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1286\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1287\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1288\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1289\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/revou-gen-ai-tutorial/venv/lib/python3.11/site-packages/anthropic/_base_client.py:1085\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1082\u001b[39m             err.response.read()\n\u001b[32m   1084\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "model.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cab95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-10 23:25:29 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-10 23:25:30 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It's always sunny in sf\n",
      "\n",
      "The\n",
      " weather\n",
      " in\n",
      " San\n",
      " Francisco\n",
      " is\n",
      " sunny\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing import Literal\n",
    "\n",
    "@tool\n",
    "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
    "    \"\"\"Use this to get weather information.\"\"\"\n",
    "    if city == \"nyc\":\n",
    "        return \"It might be cloudy in nyc\"\n",
    "    elif city == \"sf\":\n",
    "        return \"It's always sunny in sf\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown city\")\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model = \"openai:gpt-4.1-nano\",\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "async for token, metadata in agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    print(token.content)\n",
    "    #print(\"Metadata\", metadata)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5da1b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello\n",
      "!\n",
      " I\n",
      " am\n",
      " Dex\n",
      "a\n",
      " Smart\n",
      " Assistant\n",
      ".\n",
      " We\n",
      " haven't\n",
      " talked\n",
      " about\n",
      " anything\n",
      " yet\n",
      " in\n",
      " this\n",
      " conversation\n",
      ".\n",
      " How\n",
      " can\n",
      " I\n",
      " assist\n",
      " you\n",
      " today\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token, metadata in react_graph_memory.invoke({\"messages\": [HumanMessage(content=\"Hi, what have we been talking about?\")]}, config=config, stream_mode=\"messages\"):\n",
    "    print(token.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0024d",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
